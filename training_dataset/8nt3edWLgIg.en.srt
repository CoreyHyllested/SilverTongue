0
00:00:13,000 --> 00:00:15,216
I'm going to talk
about a failure of intuition

1
00:00:15,240 --> 00:00:16,840
that many of us suffer from.

2
00:00:17,480 --> 00:00:20,520
It's really a failure
to detect a certain kind of danger.

3
00:00:21,360 --> 00:00:23,096
I'm going to describe a scenario

4
00:00:23,120 --> 00:00:26,376
that I think is both terrifying

5
00:00:26,400 --> 00:00:28,160
and likely to occur,

6
00:00:28,840 --> 00:00:30,496
and that's not a good combination,

7
00:00:30,520 --> 00:00:32,056
as it turns out.

8
00:00:32,080 --> 00:00:34,536
And yet rather than be scared,
most of you will feel

9
00:00:34,560 --> 00:00:36,640
that what I'm talking about
is kind of cool.

10
00:00:37,200 --> 00:00:40,176
I'm going to describe
how the gains we make

11
00:00:40,200 --> 00:00:41,976
in artificial intelligence

12
00:00:42,000 --> 00:00:43,776
could ultimately destroy us.

13
00:00:43,800 --> 00:00:47,256
And in fact, I think it's very difficult
to see how they won't destroy us

14
00:00:47,280 --> 00:00:48,960
or inspire us to destroy ourselves.

15
00:00:49,400 --> 00:00:51,256
And yet if you're anything like me,

16
00:00:51,280 --> 00:00:53,936
you'll find that it's fun
to think about these things.

17
00:00:53,960 --> 00:00:57,336
And that response is part of the problem.

18
00:00:57,360 --> 00:00:59,080
OK? That response should worry you.

19
00:00:59,920 --> 00:01:02,576
And if I were to convince you in this talk

20
00:01:02,600 --> 00:01:06,016
that we were likely
to suffer a global famine,

21
00:01:06,040 --> 00:01:09,096
either because of climate change
or some other catastrophe,

22
00:01:09,120 --> 00:01:12,536
and that your grandchildren,
or their grandchildren,

23
00:01:12,560 --> 00:01:14,360
are very likely to live like this,

24
00:01:15,200 --> 00:01:16,400
you wouldn't think,

25
00:01:17,440 --> 00:01:18,776
"Interesting.

26
00:01:18,800 --> 00:01:20,000
I like this TED Talk."

27
00:01:21,200 --> 00:01:22,720
Famine isn't fun.

28
00:01:23,800 --> 00:01:27,176
Death by science fiction,
on the other hand, is fun,

29
00:01:27,200 --> 00:01:31,176
and one of the things that worries me most
about the development of AI at this point

30
00:01:31,200 --> 00:01:35,296
is that we seem unable to marshal
an appropriate emotional response

31
00:01:35,320 --> 00:01:37,136
to the dangers that lie ahead.

32
00:01:37,160 --> 00:01:40,360
I am unable to marshal this response,
and I'm giving this talk.

33
00:01:42,120 --> 00:01:44,816
It's as though we stand before two doors.

34
00:01:44,840 --> 00:01:46,096
Behind door number one,

35
00:01:46,120 --> 00:01:49,416
we stop making progress
in building intelligent machines.

36
00:01:49,440 --> 00:01:53,456
Our computer hardware and software
just stops getting better for some reason.

37
00:01:53,480 --> 00:01:56,480
Now take a moment
to consider why this might happen.

38
00:01:57,080 --> 00:02:00,736
I mean, given how valuable
intelligence and automation are,

39
00:02:00,760 --> 00:02:04,280
we will continue to improve our technology
if we are at all able to.

40
00:02:05,200 --> 00:02:06,867
What could stop us from doing this?

41
00:02:07,800 --> 00:02:09,600
A full-scale nuclear war?

42
00:02:11,000 --> 00:02:12,560
A global pandemic?

43
00:02:14,320 --> 00:02:15,640
An asteroid impact?

44
00:02:17,640 --> 00:02:20,216
Justin Bieber becoming
president of the United States?

45
00:02:20,240 --> 00:02:22,520
(Laughter)

46
00:02:24,760 --> 00:02:28,680
The point is, something would have to
destroy civilization as we know it.

47
00:02:29,360 --> 00:02:33,656
You have to imagine
how bad it would have to be

48
00:02:33,680 --> 00:02:37,016
to prevent us from making
improvements in our technology

49
00:02:37,040 --> 00:02:38,256
permanently,

50
00:02:38,280 --> 00:02:40,296
generation after generation.

51
00:02:40,320 --> 00:02:42,456
Almost by definition,
this is the worst thing

52
00:02:42,480 --> 00:02:44,496
that's ever happened in human history.

53
00:02:44,520 --> 00:02:45,816
So the only alternative,

54
00:02:45,840 --> 00:02:48,176
and this is what lies
behind door number two,

55
00:02:48,200 --> 00:02:51,336
is that we continue
to improve our intelligent machines

56
00:02:51,360 --> 00:02:52,960
year after year after year.

57
00:02:53,720 --> 00:02:57,360
At a certain point, we will build
machines that are smarter than we are,

58
00:02:58,080 --> 00:03:00,696
and once we have machines
that are smarter than we are,

59
00:03:00,720 --> 00:03:02,696
they will begin to improve themselves.

60
00:03:02,720 --> 00:03:05,456
And then we risk what
the mathematician IJ Good called

61
00:03:05,480 --> 00:03:07,256
an "intelligence explosion,"

62
00:03:07,280 --> 00:03:09,280
that the process could get away from us.

63
00:03:10,120 --> 00:03:12,936
Now, this is often caricatured,
as I have here,

64
00:03:12,960 --> 00:03:16,176
as a fear that armies of malicious robots

65
00:03:16,200 --> 00:03:17,456
will attack us.

66
00:03:17,480 --> 00:03:20,176
But that isn't the most likely scenario.

67
00:03:20,200 --> 00:03:25,056
It's not that our machines
will become spontaneously malevolent.

68
00:03:25,080 --> 00:03:27,696
The concern is really
that we will build machines

69
00:03:27,720 --> 00:03:29,776
that are so much
more competent than we are

70
00:03:29,800 --> 00:03:33,576
that the slightest divergence
between their goals and our own

71
00:03:33,600 --> 00:03:34,800
could destroy us.

72
00:03:35,960 --> 00:03:38,040
Just think about how we relate to ants.

73
00:03:38,600 --> 00:03:40,256
We don't hate them.

74
00:03:40,280 --> 00:03:42,336
We don't go out of our way to harm them.

75
00:03:42,360 --> 00:03:44,736
In fact, sometimes
we take pains not to harm them.

76
00:03:44,760 --> 00:03:46,776
We step over them on the sidewalk.

77
00:03:46,800 --> 00:03:48,936
But whenever their presence

78
00:03:48,960 --> 00:03:51,456
seriously conflicts with one of our goals,

79
00:03:51,480 --> 00:03:53,957
let's say when constructing
a building like this one,

80
00:03:53,981 --> 00:03:55,941
we annihilate them without a qualm.

81
00:03:56,480 --> 00:03:59,416
The concern is that we will
one day build machines

82
00:03:59,440 --> 00:04:02,176
that, whether they're conscious or not,

83
00:04:02,200 --> 00:04:04,200
could treat us with similar disregard.

84
00:04:05,760 --> 00:04:08,520
Now, I suspect this seems
far-fetched to many of you.

85
00:04:09,360 --> 00:04:15,696
I bet there are those of you who doubt
that superintelligent AI is possible,

86
00:04:15,720 --> 00:04:17,376
much less inevitable.

87
00:04:17,400 --> 00:04:21,020
But then you must find something wrong
with one of the following assumptions.

88
00:04:21,044 --> 00:04:22,616
And there are only three of them.

89
00:04:23,800 --> 00:04:28,519
Intelligence is a matter of information
processing in physical systems.

90
00:04:29,320 --> 00:04:31,935
Actually, this is a little bit more
than an assumption.

91
00:04:31,959 --> 00:04:35,416
We have already built
narrow intelligence into our machines,

92
00:04:35,440 --> 00:04:37,456
and many of these machines perform

93
00:04:37,480 --> 00:04:40,120
at a level of superhuman
intelligence already.

94
00:04:40,840 --> 00:04:43,416
And we know that mere matter

95
00:04:43,440 --> 00:04:46,056
can give rise to what is called
"general intelligence,"

96
00:04:46,080 --> 00:04:49,736
an ability to think flexibly
across multiple domains,

97
00:04:49,760 --> 00:04:52,896
because our brains have managed it. Right?

98
00:04:52,920 --> 00:04:56,856
I mean, there's just atoms in here,

99
00:04:56,880 --> 00:05:01,376
and as long as we continue
to build systems of atoms

100
00:05:01,400 --> 00:05:04,096
that display more and more
intelligent behavior,

101
00:05:04,120 --> 00:05:06,656
we will eventually,
unless we are interrupted,

102
00:05:06,680 --> 00:05:10,056
we will eventually
build general intelligence

103
00:05:10,080 --> 00:05:11,376
into our machines.

104
00:05:11,400 --> 00:05:15,056
It's crucial to realize
that the rate of progress doesn't matter,

105
00:05:15,080 --> 00:05:18,256
because any progress
is enough to get us into the end zone.

106
00:05:18,280 --> 00:05:22,056
We don't need Moore's law to continue.
We don't need exponential progress.

107
00:05:22,080 --> 00:05:23,680
We just need to keep going.

108
00:05:25,480 --> 00:05:28,400
The second assumption
is that we will keep going.

109
00:05:29,000 --> 00:05:31,760
We will continue to improve
our intelligent machines.

110
00:05:33,000 --> 00:05:37,376
And given the value of intelligence --

111
00:05:37,400 --> 00:05:40,936
I mean, intelligence is either
the source of everything we value

112
00:05:40,960 --> 00:05:43,736
or we need it to safeguard
everything we value.

113
00:05:43,760 --> 00:05:46,016
It is our most valuable resource.

114
00:05:46,040 --> 00:05:47,576
So we want to do this.

115
00:05:47,600 --> 00:05:50,936
We have problems
that we desperately need to solve.

116
00:05:50,960 --> 00:05:54,160
We want to cure diseases
like Alzheimer's and cancer.

117
00:05:54,960 --> 00:05:58,896
We want to understand economic systems.
We want to improve our climate science.

118
00:05:58,920 --> 00:06:01,176
So we will do this, if we can.

119
00:06:01,200 --> 00:06:04,486
The train is already out of the station,
and there's no brake to pull.

120
00:06:05,880 --> 00:06:11,336
Finally, we don't stand
on a peak of intelligence,

121
00:06:11,360 --> 00:06:13,160
or anywhere near it, likely.

122
00:06:13,640 --> 00:06:15,536
And this really is the crucial insight.

123
00:06:15,560 --> 00:06:17,976
This is what makes
our situation so precarious,

124
00:06:18,000 --> 00:06:22,040
and this is what makes our intuitions
about risk so unreliable.

125
00:06:23,120 --> 00:06:25,840
Now, just consider the smartest person
who has ever lived.

126
00:06:26,640 --> 00:06:30,056
On almost everyone's shortlist here
is John von Neumann.

127
00:06:30,080 --> 00:06:33,416
I mean, the impression that von Neumann
made on the people around him,

128
00:06:33,440 --> 00:06:37,496
and this included the greatest
mathematicians and physicists of his time,

129
00:06:37,520 --> 00:06:39,456
is fairly well-documented.

130
00:06:39,480 --> 00:06:43,256
If only half the stories
about him are half true,

131
00:06:43,280 --> 00:06:44,496
there's no question

132
00:06:44,520 --> 00:06:46,976
he's one of the smartest people
who has ever lived.

133
00:06:47,000 --> 00:06:49,520
So consider the spectrum of intelligence.

134
00:06:50,320 --> 00:06:51,749
Here we have John von Neumann.

135
00:06:53,560 --> 00:06:54,894
And then we have you and me.

136
00:06:56,120 --> 00:06:57,416
And then we have a chicken.

137
00:06:57,440 --> 00:06:59,376
(Laughter)

138
00:06:59,400 --> 00:07:00,616
Sorry, a chicken.

139
00:07:00,640 --> 00:07:01,896
(Laughter)

140
00:07:01,920 --> 00:07:05,656
There's no reason for me to make this talk
more depressing than it needs to be.

141
00:07:05,680 --> 00:07:07,280
(Laughter)

142
00:07:08,339 --> 00:07:11,816
It seems overwhelmingly likely, however,
that the spectrum of intelligence

143
00:07:11,840 --> 00:07:14,960
extends much further
than we currently conceive,

144
00:07:15,880 --> 00:07:19,096
and if we build machines
that are more intelligent than we are,

145
00:07:19,120 --> 00:07:21,416
they will very likely
explore this spectrum

146
00:07:21,440 --> 00:07:23,296
in ways that we can't imagine,

147
00:07:23,320 --> 00:07:25,840
and exceed us in ways
that we can't imagine.

148
00:07:27,000 --> 00:07:31,336
And it's important to recognize that
this is true by virtue of speed alone.

149
00:07:31,360 --> 00:07:36,416
Right? So imagine if we just built
a superintelligent AI

150
00:07:36,440 --> 00:07:39,896
that was no smarter
than your average team of researchers

151
00:07:39,920 --> 00:07:42,216
at Stanford or MIT.

152
00:07:42,240 --> 00:07:45,216
Well, electronic circuits
function about a million times faster

153
00:07:45,240 --> 00:07:46,496
than biochemical ones,

154
00:07:46,520 --> 00:07:49,656
so this machine should think
about a million times faster

155
00:07:49,680 --> 00:07:51,496
than the minds that built it.

156
00:07:51,520 --> 00:07:53,176
So you set it running for a week,

157
00:07:53,200 --> 00:07:57,760
and it will perform 20,000 years
of human-level intellectual work,

158
00:07:58,400 --> 00:08:00,360
week after week after week.

159
00:08:01,640 --> 00:08:04,736
How could we even understand,
much less constrain,

160
00:08:04,760 --> 00:08:07,040
a mind making this sort of progress?

161
00:08:08,840 --> 00:08:10,976
The other thing that's worrying, frankly,

162
00:08:11,000 --> 00:08:15,976
is that, imagine the best case scenario.

163
00:08:16,000 --> 00:08:20,176
So imagine we hit upon a design
of superintelligent AI

164
00:08:20,200 --> 00:08:21,576
that has no safety concerns.

165
00:08:21,600 --> 00:08:24,856
We have the perfect design
the first time around.

166
00:08:24,880 --> 00:08:27,096
It's as though we've been handed an oracle

167
00:08:27,120 --> 00:08:29,136
that behaves exactly as intended.

168
00:08:29,160 --> 00:08:32,880
Well, this machine would be
the perfect labor-saving device.

169
00:08:33,680 --> 00:08:36,109
It can design the machine
that can build the machine

170
00:08:36,133 --> 00:08:37,896
that can do any physical work,

171
00:08:37,920 --> 00:08:39,376
powered by sunlight,

172
00:08:39,400 --> 00:08:42,096
more or less for the cost
of raw materials.

173
00:08:42,120 --> 00:08:45,376
So we're talking about
the end of human drudgery.

174
00:08:45,400 --> 00:08:48,200
We're also talking about the end
of most intellectual work.

175
00:08:49,200 --> 00:08:52,256
So what would apes like ourselves
do in this circumstance?

176
00:08:52,280 --> 00:08:56,360
Well, we'd be free to play Frisbee
and give each other massages.

177
00:08:57,840 --> 00:09:00,696
Add some LSD and some
questionable wardrobe choices,

178
00:09:00,720 --> 00:09:02,896
and the whole world
could be like Burning Man.

179
00:09:02,920 --> 00:09:04,560
(Laughter)

180
00:09:06,320 --> 00:09:08,320
Now, that might sound pretty good,

181
00:09:09,280 --> 00:09:11,656
but ask yourself what would happen

182
00:09:11,680 --> 00:09:14,416
under our current economic
and political order?

183
00:09:14,440 --> 00:09:16,856
It seems likely that we would witness

184
00:09:16,880 --> 00:09:21,016
a level of wealth inequality
and unemployment

185
00:09:21,040 --> 00:09:22,536
that we have never seen before.

186
00:09:22,560 --> 00:09:25,176
Absent a willingness
to immediately put this new wealth

187
00:09:25,200 --> 00:09:26,680
to the service of all humanity,

188
00:09:27,640 --> 00:09:31,256
a few trillionaires could grace
the covers of our business magazines

189
00:09:31,280 --> 00:09:33,720
while the rest of the world
would be free to starve.

190
00:09:34,320 --> 00:09:36,616
And what would the Russians
or the Chinese do

191
00:09:36,640 --> 00:09:39,256
if they heard that some company
in Silicon Valley

192
00:09:39,280 --> 00:09:42,016
was about to deploy a superintelligent AI?

193
00:09:42,040 --> 00:09:44,896
This machine would be capable
of waging war,

194
00:09:44,920 --> 00:09:47,136
whether terrestrial or cyber,

195
00:09:47,160 --> 00:09:48,840
with unprecedented power.

196
00:09:50,120 --> 00:09:51,976
This is a winner-take-all scenario.

197
00:09:52,000 --> 00:09:55,136
To be six months ahead
of the competition here

198
00:09:55,160 --> 00:09:57,936
is to be 500,000 years ahead,

199
00:09:57,960 --> 00:09:59,456
at a minimum.

200
00:09:59,480 --> 00:10:04,216
So it seems that even mere rumors
of this kind of breakthrough

201
00:10:04,240 --> 00:10:06,616
could cause our species to go berserk.

202
00:10:06,640 --> 00:10:09,536
Now, one of the most frightening things,

203
00:10:09,560 --> 00:10:12,336
in my view, at this moment,

204
00:10:12,360 --> 00:10:16,656
are the kinds of things
that AI researchers say

205
00:10:16,680 --> 00:10:18,240
when they want to be reassuring.

206
00:10:19,000 --> 00:10:22,456
And the most common reason
we're told not to worry is time.

207
00:10:22,480 --> 00:10:24,536
This is all a long way off,
don't you know.

208
00:10:24,560 --> 00:10:27,000
This is probably 50 or 100 years away.

209
00:10:27,720 --> 00:10:28,976
One researcher has said,

210
00:10:29,000 --> 00:10:30,576
"Worrying about AI safety

211
00:10:30,600 --> 00:10:32,880
is like worrying
about overpopulation on Mars."

212
00:10:34,116 --> 00:10:35,736
This is the Silicon Valley version

213
00:10:35,760 --> 00:10:38,136
of "don't worry your
pretty little head about it."

214
00:10:38,160 --> 00:10:39,496
(Laughter)

215
00:10:39,520 --> 00:10:41,416
No one seems to notice

216
00:10:41,440 --> 00:10:44,056
that referencing the time horizon

217
00:10:44,080 --> 00:10:46,656
is a total non sequitur.

218
00:10:46,680 --> 00:10:49,936
If intelligence is just a matter
of information processing,

219
00:10:49,960 --> 00:10:52,616
and we continue to improve our machines,

220
00:10:52,640 --> 00:10:55,520
we will produce
some form of superintelligence.

221
00:10:56,320 --> 00:10:59,976
And we have no idea
how long it will take us

222
00:11:00,000 --> 00:11:02,400
to create the conditions
to do that safely.

223
00:11:04,200 --> 00:11:05,496
Let me say that again.

224
00:11:05,520 --> 00:11:09,336
We have no idea how long it will take us

225
00:11:09,360 --> 00:11:11,600
to create the conditions
to do that safely.

226
00:11:12,920 --> 00:11:16,376
And if you haven't noticed,
50 years is not what it used to be.

227
00:11:16,400 --> 00:11:18,856
This is 50 years in months.

228
00:11:18,880 --> 00:11:20,720
This is how long we've had the iPhone.

229
00:11:21,440 --> 00:11:24,040
This is how long "The Simpsons"
has been on television.

230
00:11:24,680 --> 00:11:27,056
Fifty years is not that much time

231
00:11:27,080 --> 00:11:30,240
to meet one of the greatest challenges
our species will ever face.

232
00:11:31,640 --> 00:11:35,656
Once again, we seem to be failing
to have an appropriate emotional response

233
00:11:35,680 --> 00:11:38,376
to what we have every reason
to believe is coming.

234
00:11:38,400 --> 00:11:42,376
The computer scientist Stuart Russell
has a nice analogy here.

235
00:11:42,400 --> 00:11:47,296
He said, imagine that we received
a message from an alien civilization,

236
00:11:47,320 --> 00:11:49,016
which read:

237
00:11:49,040 --> 00:11:50,576
"People of Earth,

238
00:11:50,600 --> 00:11:52,960
we will arrive on your planet in 50 years.

239
00:11:53,800 --> 00:11:55,376
Get ready."

240
00:11:55,400 --> 00:11:59,656
And now we're just counting down
the months until the mothership lands?

241
00:11:59,680 --> 00:12:02,680
We would feel a little
more urgency than we do.

242
00:12:04,680 --> 00:12:06,536
Another reason we're told not to worry

243
00:12:06,560 --> 00:12:09,576
is that these machines
can't help but share our values

244
00:12:09,600 --> 00:12:12,216
because they will be literally
extensions of ourselves.

245
00:12:12,240 --> 00:12:14,056
They'll be grafted onto our brains,

246
00:12:14,080 --> 00:12:16,440
and we'll essentially
become their limbic systems.

247
00:12:17,120 --> 00:12:18,536
Now take a moment to consider

248
00:12:18,560 --> 00:12:21,736
that the safest
and only prudent path forward,

249
00:12:21,760 --> 00:12:23,096
recommended,

250
00:12:23,120 --> 00:12:25,920
is to implant this technology
directly into our brains.

251
00:12:26,600 --> 00:12:29,976
Now, this may in fact be the safest
and only prudent path forward,

252
00:12:30,000 --> 00:12:33,056
but usually one's safety concerns
about a technology

253
00:12:33,080 --> 00:12:36,736
have to be pretty much worked out
before you stick it inside your head.

254
00:12:36,760 --> 00:12:38,776
(Laughter)

255
00:12:38,800 --> 00:12:44,136
The deeper problem is that
building superintelligent AI on its own

256
00:12:44,160 --> 00:12:45,896
seems likely to be easier

257
00:12:45,920 --> 00:12:47,776
than building superintelligent AI

258
00:12:47,800 --> 00:12:49,576
and having the completed neuroscience

259
00:12:49,600 --> 00:12:52,280
that allows us to seamlessly
integrate our minds with it.

260
00:12:52,800 --> 00:12:55,976
And given that the companies
and governments doing this work

261
00:12:56,000 --> 00:12:59,656
are likely to perceive themselves
as being in a race against all others,

262
00:12:59,680 --> 00:13:02,936
given that to win this race
is to win the world,

263
00:13:02,960 --> 00:13:05,416
provided you don't destroy it
in the next moment,

264
00:13:05,440 --> 00:13:08,056
then it seems likely
that whatever is easier to do

265
00:13:08,080 --> 00:13:09,280
will get done first.

266
00:13:10,560 --> 00:13:13,416
Now, unfortunately,
I don't have a solution to this problem,

267
00:13:13,440 --> 00:13:16,056
apart from recommending
that more of us think about it.

268
00:13:16,080 --> 00:13:18,456
I think we need something
like a Manhattan Project

269
00:13:18,480 --> 00:13:20,496
on the topic of artificial intelligence.

270
00:13:20,520 --> 00:13:23,256
Not to build it, because I think
we'll inevitably do that,

271
00:13:23,280 --> 00:13:26,616
but to understand
how to avoid an arms race

272
00:13:26,640 --> 00:13:30,136
and to build it in a way
that is aligned with our interests.

273
00:13:30,160 --> 00:13:32,296
When you're talking
about superintelligent AI

274
00:13:32,320 --> 00:13:34,576
that can make changes to itself,

275
00:13:34,600 --> 00:13:39,216
it seems that we only have one chance
to get the initial conditions right,

276
00:13:39,240 --> 00:13:41,296
and even then we will need to absorb

277
00:13:41,320 --> 00:13:44,360
the economic and political
consequences of getting them right.

278
00:13:45,760 --> 00:13:47,816
But the moment we admit

279
00:13:47,840 --> 00:13:51,840
that information processing
is the source of intelligence,

280
00:13:52,720 --> 00:13:57,520
that some appropriate computational system
is what the basis of intelligence is,

281
00:13:58,360 --> 00:14:02,120
and we admit that we will improve
these systems continuously,

282
00:14:03,280 --> 00:14:07,736
and we admit that the horizon
of cognition very likely far exceeds

283
00:14:07,760 --> 00:14:08,960
what we currently know,

284
00:14:10,120 --> 00:14:11,336
then we have to admit

285
00:14:11,360 --> 00:14:14,000
that we are in the process
of building some sort of god.

286
00:14:15,400 --> 00:14:16,976
Now would be a good time

287
00:14:17,000 --> 00:14:18,953
to make sure it's a god we can live with.

288
00:14:20,120 --> 00:14:21,656
Thank you very much.

289
00:14:21,680 --> 00:14:26,773
(Applause)
